# -*- coding: utf-8 -*-
"""6.Codigo completo_Optuna _VotingClassifier_Evaluacion Soft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JcA3BDb_L8P5hOtPMEvL6n_NDos-MbNu
"""

#@title 6.Código completo: Optuna + VotingClassifier + Evaluación Soft
# 1. Instalación
#!pip install optuna catboost xgboost lightgbm --quiet

# 2. Imports
#import optuna
#from xgboost import XGBClassifier
#from catboost import CatBoostClassifier
#from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
#from sklearn.model_selection import train_test_split
#from sklearn.ensemble import VotingClassifier
#import pandas as pd

# 3. Reducir conjunto para optimización (más rápido)
X_train_opt, _, y_train_opt, _ = train_test_split(X_train_final, y_train_final, train_size=0.5, random_state=42)

# 4. Función Optuna
def objective(trial, model_name):
    if model_name == "XGBoost":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 300),
            "max_depth": trial.suggest_int("max_depth", 3, 6),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.5, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            "scale_pos_weight": trial.suggest_float("scale_pos_weight", 1.0, 3.0),
            "eval_metric": "auc",
            "use_label_encoder": False,
            "random_state": 42,
            "verbosity": 0
        }
        model = XGBClassifier(**params)

    elif model_name == "CatBoost":
        params = {
            "iterations": trial.suggest_int("iterations", 100, 300),
            "depth": trial.suggest_int("depth", 3, 6),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 5),
            "random_state": 42,
            "verbose": 0
        }
        model = CatBoostClassifier(**params)

    model.fit(X_train_opt, y_train_opt)
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    return roc_auc_score(y_test, y_proba)

# 5. Optimización
studies = {}
for modelo in ["XGBoost", "CatBoost"]:
    study = optuna.create_study(direction="maximize")
    study.optimize(lambda trial: objective(trial, modelo), n_trials=25, n_jobs=-1)
    studies[modelo] = study
    print(f"\n Mejor configuración para {modelo}:")
    print(study.best_params)

# 6. Recuperar mejores parámetros
best_params_xgb = studies["XGBoost"].best_params
best_params_cat = studies["CatBoost"].best_params

# 7. VotingClassifier con mejores parámetros
voting_model = VotingClassifier(
    estimators=[
        ('xgb', XGBClassifier(**best_params_xgb, eval_metric='auc', use_label_encoder=False, random_state=42)),
        ('cat', CatBoostClassifier(**best_params_cat, verbose=0, random_state=42))
    ],
    voting='soft' # Cambiar a 'hard' si deseas votación dura
)

voting_model.fit(X_train_final, y_train_final)

# 8. Evaluar con umbral ajustado
threshold = 0.57  # puedes cambiar este valor si lo deseas
y_proba = voting_model.predict_proba(X_test_scaled)[:, 1]
y_pred = (y_proba >= threshold).astype(int)

# 9. Métricas
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)

print(f"\n VotingClassifier (XGB + CatBoost, umbral = {threshold})")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-score:  {f1:.4f}")
print(f"ROC AUC:   {roc_auc:.4f}")